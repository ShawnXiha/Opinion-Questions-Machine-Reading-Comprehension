{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import pickle\n",
    "import json\n",
    "import jieba\n",
    "from tqdm import tqdm\n",
    "\n",
    "def seg_line(line):\n",
    "    return list(jieba.cut(line))\n",
    "\n",
    "\n",
    "def seg_data(path):\n",
    "    print('start process ', path)\n",
    "    data = []\n",
    "    with open(path, 'r') as f:\n",
    "        for line in tqdm(f):\n",
    "            dic = json.loads(line, encoding='utf-8')\n",
    "            question = dic['query']\n",
    "            doc = dic['passage']\n",
    "            alternatives = dic['alternatives']\n",
    "            data.append([seg_line(question), seg_line(doc), alternatives.split('|'), dic['query_id']])\n",
    "    return data\n",
    "\n",
    "\n",
    "def build_word_count(data):\n",
    "    wordCount = {}\n",
    "\n",
    "    def add_count(lst):\n",
    "        for word in lst:\n",
    "            if word not in wordCount:\n",
    "                wordCount[word] = 0\n",
    "            wordCount[word] += 1\n",
    "\n",
    "    for one in data:\n",
    "        [add_count(x) for x in one[0:3]]\n",
    "    print('word type size ', len(wordCount))\n",
    "    return wordCount\n",
    "\n",
    "\n",
    "def build_word2id(wordCount, threshold=10):\n",
    "    word2id = {'<PAD>': 0, '<UNK>': 1}\n",
    "    for word in wordCount:\n",
    "        if wordCount[word] >= threshold:\n",
    "            if word not in word2id:\n",
    "                word2id[word] = len(word2id)\n",
    "        else:\n",
    "            chars = list(word)\n",
    "            for char in chars:\n",
    "                if char not in word2id:\n",
    "                    word2id[char] = len(word2id)\n",
    "    print('processed word size ', len(word2id))\n",
    "    return word2id\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data_to_id(raw_data, word2id):\n",
    "    data = []\n",
    "\n",
    "    def map_word_to_id(word):\n",
    "        output = []\n",
    "        if word in word2id:\n",
    "            output.append(word2id[word])\n",
    "        else:\n",
    "            chars = list(word)\n",
    "            for char in chars:\n",
    "                if char in word2id:\n",
    "                    output.append(word2id[char])\n",
    "                else:\n",
    "                    output.append(1)\n",
    "        return output\n",
    "\n",
    "    def map_sent_to_id(sent):\n",
    "        output = []\n",
    "        for word in sent:\n",
    "            output.extend(map_word_to_id(word))\n",
    "        return output\n",
    "\n",
    "    for one in raw_data:\n",
    "        question = map_sent_to_id(one[0])\n",
    "        doc = map_sent_to_id(one[1])\n",
    "        candidates = [map_word_to_id(x) for x in one[2]]\n",
    "        length = [len(x) for x in candidates]\n",
    "        max_length = max(length)\n",
    "        if max_length > 1:\n",
    "            pad_len = [max_length - x for x in length]\n",
    "            candidates = [x[0] + [0] * x[1] for x in zip(candidates, pad_len)]\n",
    "        data.append([question, doc, candidates, one[-1]])\n",
    "    return data\n",
    "\n",
    "\n",
    "def process_data(data_path, threshold):\n",
    "    train_file_path = data_path + 'ai_challenger_oqmrc_validationset_20180816/ai_challenger_oqmrc_validationset.json'\n",
    "    dev_file_path = data_path + 'ai_challenger_oqmrc_trainingset_20180816/ai_challenger_oqmrc_trainingset.json'\n",
    "    test_a_file_path = data_path + 'ai_challenger_oqmrc_testa_20180816/ai_challenger_oqmrc_testa.json'\n",
    "    # test_b_file_path = data_path + 'ai_challenger_oqmrc_testb_20180816/ai_challenger_oqmrc_testb.json'\n",
    "    path_lst = [train_file_path, dev_file_path, test_a_file_path]\n",
    "    output_path = [data_path + x for x in ['dev.pickle', 'train.pickle', 'testa.pickle']]\n",
    "    return _process_data(path_lst, threshold, output_path)\n",
    "\n",
    "\n",
    "def _process_data(path_lst, word_min_count=5, output_file_path=[]):\n",
    "    raw_data = []\n",
    "    for path in path_lst:\n",
    "        raw_data.append(seg_data(path))\n",
    "    word_count = build_word_count([y for x in raw_data for y in x])\n",
    "    with open('../inputs//word-count.obj', 'wb') as f:\n",
    "        pickle.dump(word_count, f)\n",
    "    word2id = build_word2id(word_count, word_min_count)\n",
    "    with open('../inputs//word2id.obj', 'wb') as f:\n",
    "        pickle.dump(word2id, f)\n",
    "    for one_raw_data, one_output_file_path in zip(raw_data, output_file_path):\n",
    "        with open(one_output_file_path, 'wb') as f:\n",
    "            one_data = transform_data_to_id(one_raw_data, word2id)\n",
    "            pickle.dump(one_data, f)\n",
    "    return len(word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    " process_data(\"/home/xq/data/aichallenger/Opinion Questions Machine Reading Comprehension/\", 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def pad_answer(batch):\n",
    "    output = []\n",
    "    length_info = [len(x[0]) for x in batch]\n",
    "    max_length = max(length_info)\n",
    "    for one in batch:\n",
    "        output.append([x + [0] * (max_length - len(x)) for x in one])\n",
    "    return output\n",
    "\n",
    "\n",
    "def get_model_parameters(model):\n",
    "    total = 0\n",
    "    for parameter in model.parameters():\n",
    "        if parameter.requires_grad:\n",
    "            tmp = 1\n",
    "            for a in parameter.size():\n",
    "                tmp *= a\n",
    "            total += tmp\n",
    "    return total\n",
    "\n",
    "\n",
    "def padding(sequence, pads=0, max_len=None, dtype='int32', return_matrix_for_size=False):\n",
    "    # we should judge the rank\n",
    "    if True or isinstance(sequence[0], list):\n",
    "        v_length = [len(x) for x in sequence]  # every sequence length\n",
    "        seq_max_len = max(v_length)\n",
    "        if (max_len is None) or (max_len > seq_max_len):\n",
    "            max_len = seq_max_len\n",
    "        v_length = list(map(lambda z: z if z <= max_len else max_len, v_length))\n",
    "        x = (np.ones((len(sequence), max_len)) * pads).astype(dtype)\n",
    "        for idx, s in enumerate(sequence):\n",
    "            trunc = s[:max_len]\n",
    "            x[idx, :len(trunc)] = trunc\n",
    "        if return_matrix_for_size:\n",
    "            v_matrix = np.asanyarray([map(lambda item: 1 if item < line else 0, range(max_len)) for line in v_length],\n",
    "                                     dtype=dtype)\n",
    "            return x, v_matrix\n",
    "        return x, np.asarray(v_length, dtype='int32')\n",
    "    else:\n",
    "        seq_len = len(sequence)\n",
    "        if max_len is None:\n",
    "            max_len = seq_len\n",
    "        v_vector = sequence + [0] * (max_len - seq_len)\n",
    "        padded_vector = np.asarray(v_vector, dtype=dtype)\n",
    "        v_index = [1] * seq_len + [0] * (max_len - seq_len)\n",
    "        padded_index = np.asanyarray(v_index, dtype=dtype)\n",
    "        return padded_vector, padded_index\n",
    "\n",
    "\n",
    "def shuffle_data(data, axis=1):\n",
    "    pool = {}\n",
    "    for one in data:\n",
    "        length = len(one[axis])\n",
    "        if length not in pool:\n",
    "            pool[length] = []\n",
    "        pool[length].append(one)\n",
    "    for one in pool:\n",
    "        np.random.shuffle(pool[one])\n",
    "    length_lst = list(pool.keys())\n",
    "    np.random.shuffle(length_lst)\n",
    "    return [x for y in length_lst for x in pool[y]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class MwAN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, encoder_size, drop_out=0.2):\n",
    "        super(MwAN, self).__init__()\n",
    "        self.drop_out=drop_out\n",
    "        self.embedding = nn.Embedding(vocab_size + 1, embedding_dim=embedding_size)\n",
    "        self.q_encoder = nn.GRU(input_size=embedding_size, hidden_size=encoder_size, batch_first=True,\n",
    "                                bidirectional=True)\n",
    "        self.p_encoder = nn.GRU(input_size=embedding_size, hidden_size=encoder_size, batch_first=True,\n",
    "                                bidirectional=True)\n",
    "        self.a_encoder = nn.GRU(input_size=embedding_size, hidden_size=embedding_size // 2, batch_first=True,\n",
    "                                bidirectional=True)\n",
    "        self.a_attention = nn.Linear(embedding_size, 1, bias=False)\n",
    "        # Concat Attention\n",
    "        self.Wc1 = nn.Linear(2 * encoder_size, encoder_size, bias=False)\n",
    "        self.Wc2 = nn.Linear(2 * encoder_size, encoder_size, bias=False)\n",
    "        self.vc = nn.Linear(encoder_size, 1, bias=False)\n",
    "        # Bilinear Attention\n",
    "        self.Wb = nn.Linear(2 * encoder_size, 2 * encoder_size, bias=False)\n",
    "        # Dot Attention :\n",
    "        self.Wd = nn.Linear(2 * encoder_size, encoder_size, bias=False)\n",
    "        self.vd = nn.Linear(encoder_size, 1, bias=False)\n",
    "        # Minus Attention :\n",
    "        self.Wm = nn.Linear(2 * encoder_size, encoder_size, bias=False)\n",
    "        self.vm = nn.Linear(encoder_size, 1, bias=False)\n",
    "\n",
    "        self.Ws = nn.Linear(2 * encoder_size, encoder_size, bias=False)\n",
    "        self.vs = nn.Linear(encoder_size, 1, bias=False)\n",
    "\n",
    "        self.gru_agg = nn.GRU(12 * encoder_size, encoder_size, batch_first=True, bidirectional=True)\n",
    "        \"\"\"\n",
    "        prediction layer\n",
    "        \"\"\"\n",
    "        self.Wq = nn.Linear(2 * encoder_size, encoder_size, bias=False)\n",
    "        self.vq = nn.Linear(encoder_size, 1, bias=False)\n",
    "        self.Wp1 = nn.Linear(2 * encoder_size, encoder_size, bias=False)\n",
    "        self.Wp2 = nn.Linear(2 * encoder_size, encoder_size, bias=False)\n",
    "        self.vp = nn.Linear(encoder_size, 1, bias=False)\n",
    "        self.prediction = nn.Linear(2 * encoder_size, embedding_size, bias=False)\n",
    "        self.initiation()\n",
    "\n",
    "    def initiation(self):\n",
    "        initrange = 0.1\n",
    "        nn.init.uniform_(self.embedding.weight, -initrange, initrange)\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.xavier_uniform_(module.weight, 0.1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        [query, passage, answer, is_train] = inputs\n",
    "        q_embedding = self.embedding(query)\n",
    "        p_embedding = self.embedding(passage)\n",
    "        a_embeddings = self.embedding(answer)\n",
    "        a_embedding, _ = self.a_encoder(a_embeddings.view(-1, a_embeddings.size(2), a_embeddings.size(3)))\n",
    "        a_score = F.softmax(self.a_attention(a_embedding), 1)\n",
    "        a_output = a_score.transpose(2, 1).bmm(a_embedding).squeeze()\n",
    "        a_embedding = a_output.view(a_embeddings.size(0), 3, -1)\n",
    "        hq, _ = self.q_encoder(p_embedding)\n",
    "        hq=F.dropout(hq,self.drop_out)\n",
    "        hp, _ = self.p_encoder(q_embedding)\n",
    "        hp=F.dropout(hp,self.drop_out)\n",
    "        _s1 = self.Wc1(hq).unsqueeze(1)\n",
    "        _s2 = self.Wc2(hp).unsqueeze(2)\n",
    "        sjt = self.vc(torch.tanh(_s1 + _s2)).squeeze()\n",
    "        ait = F.softmax(sjt, 2)\n",
    "        qtc = ait.bmm(hq)\n",
    "        _s1 = self.Wb(hq).transpose(2, 1)\n",
    "        sjt = hp.bmm(_s1)\n",
    "        ait = F.softmax(sjt, 2)\n",
    "        qtb = ait.bmm(hq)\n",
    "        _s1 = hq.unsqueeze(1)\n",
    "        _s2 = hp.unsqueeze(2)\n",
    "        sjt = self.vd(torch.tanh(self.Wd(_s1 * _s2))).squeeze()\n",
    "        ait = F.softmax(sjt, 2)\n",
    "        qtd = ait.bmm(hq)\n",
    "        sjt = self.vm(torch.tanh(self.Wm(_s1 - _s2))).squeeze()\n",
    "        ait = F.softmax(sjt, 2)\n",
    "        qtm = ait.bmm(hq)\n",
    "        _s1 = hp.unsqueeze(1)\n",
    "        _s2 = hp.unsqueeze(2)\n",
    "        sjt = self.vs(torch.tanh(self.Ws(_s1 * _s2))).squeeze()\n",
    "        ait = F.softmax(sjt, 2)\n",
    "        qts = ait.bmm(hp)\n",
    "        aggregation = torch.cat([hp, qts, qtc, qtd, qtb, qtm], 2)\n",
    "        aggregation_representation, _ = self.gru_agg(aggregation)\n",
    "        sj = self.vq(torch.tanh(self.Wq(hq))).transpose(2, 1)\n",
    "        rq = F.softmax(sj, 2).bmm(hq)\n",
    "        sj = F.softmax(self.vp(self.Wp1(aggregation_representation) + self.Wp2(rq)).transpose(2, 1), 2)\n",
    "        rp = sj.bmm(aggregation_representation)\n",
    "        encoder_output = F.dropout(F.leaky_relu(self.prediction(rp)),self.drop_out)\n",
    "        score = F.softmax(a_embedding.bmm(encoder_output.transpose(2, 1)).squeeze(), 1)\n",
    "        if not is_train:\n",
    "            return score.argmax(1)\n",
    "        loss = -torch.log(score[:, 0]).mean()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model total parameters: 14524288\n",
      "train data size 250000, dev data size 30000\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import torch\n",
    "\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.data = '/home/xq/data/aichallenger/Opinion Questions Machine Reading Comprehension/'\n",
    "        self.threshold = 5\n",
    "        self.epoch = 50\n",
    "        self.emsize = 128\n",
    "        self.nhid = 128\n",
    "        self.batch_size = 64\n",
    "        self.log_interval = 300\n",
    "        self.dropout = 0.2\n",
    "        self.cuda = True\n",
    "        self.save = \"../ckpt/model.pt\"\n",
    "\n",
    "\n",
    "args = Config()\n",
    "\n",
    "# vocab_size = process_data(args.data, args.threshold)\n",
    "vocab_size = 96973\n",
    "\n",
    "model = MwAN(vocab_size=vocab_size, embedding_size=args.emsize, encoder_size=args.nhid, drop_out=args.dropout)\n",
    "print('Model total parameters:', get_model_parameters(model))\n",
    "if args.cuda:\n",
    "    model.cuda()\n",
    "optimizer = torch.optim.Adamax(model.parameters())\n",
    "\n",
    "with open(args.data + 'train.pickle', 'rb') as f:\n",
    "    train_data = pickle.load(f)\n",
    "with open(args.data + 'dev.pickle', 'rb') as f:\n",
    "    dev_data = pickle.load(f)\n",
    "dev_data = sorted(dev_data, key=lambda x: len(x[1]))\n",
    "\n",
    "print('train data size {:d}, dev data size {:d}'.format(len(train_data), len(dev_data)))\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    data = shuffle_data(train_data, 1)\n",
    "    total_loss = 0.0\n",
    "    for num, i in enumerate(range(0, len(data), args.batch_size)):\n",
    "        one = data[i:i + args.batch_size]\n",
    "        query, _ = padding([x[0] for x in one], max_len=50)\n",
    "        passage, _ = padding([x[1] for x in one], max_len=350)\n",
    "        answer = pad_answer([x[2] for x in one])\n",
    "        query, passage, answer = torch.LongTensor(query), torch.LongTensor(passage), torch.LongTensor(answer)\n",
    "        if args.cuda:\n",
    "            query = query.cuda()\n",
    "            passage = passage.cuda()\n",
    "            answer = answer.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        loss = model([query, passage, answer, True])\n",
    "        loss.backward()\n",
    "        total_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        if (num + 1) % args.log_interval == 0:\n",
    "            print('|------epoch {:d} train error is {:f}  eclipse {:.2f}%------|'.format(epoch,\n",
    "                                                                                         total_loss / args.log_interval,\n",
    "                                                                                         i * 100.0 / len(data)))\n",
    "            total_loss = 0\n",
    "\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    r, a = 0.0, 0.0\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(dev_data), args.batch_size):\n",
    "            one = dev_data[i:i + args.batch_size]\n",
    "            query, _ = padding([x[0] for x in one], max_len=50)\n",
    "            passage, _ = padding([x[1] for x in one], max_len=500)\n",
    "            answer = pad_answer([x[2] for x in one])\n",
    "            query, passage, answer = torch.LongTensor(query), torch.LongTensor(passage), torch.LongTensor(answer)\n",
    "            if args.cuda:\n",
    "                query = query.cuda()\n",
    "                passage = passage.cuda()\n",
    "                answer = answer.cuda()\n",
    "            output = model([query, passage, answer, False])\n",
    "            r += torch.eq(output, 0).sum().item()\n",
    "            a += len(one)\n",
    "    return r * 100.0 / a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|------epoch 0 train error is 0.936349  eclipse 7.65%------|\n",
      "|------epoch 0 train error is 0.929350  eclipse 15.33%------|\n",
      "|------epoch 0 train error is 0.876534  eclipse 23.01%------|\n",
      "|------epoch 0 train error is 0.881147  eclipse 30.69%------|\n",
      "|------epoch 0 train error is 0.818682  eclipse 38.37%------|\n",
      "|------epoch 0 train error is 0.820160  eclipse 46.05%------|\n",
      "|------epoch 0 train error is 0.787513  eclipse 53.73%------|\n",
      "|------epoch 0 train error is 0.804186  eclipse 61.41%------|\n",
      "|------epoch 0 train error is 0.806545  eclipse 69.09%------|\n",
      "|------epoch 0 train error is 0.777462  eclipse 76.77%------|\n",
      "|------epoch 0 train error is 0.768388  eclipse 84.45%------|\n",
      "|------epoch 0 train error is 0.719827  eclipse 92.13%------|\n",
      "|------epoch 0 train error is 0.750242  eclipse 99.81%------|\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xq/anaconda3/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type MwAN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epcoh 0 dev acc is 66.206667, best dev acc 66.206667\n",
      "|------epoch 1 train error is 0.775766  eclipse 7.65%------|\n",
      "|------epoch 1 train error is 0.685328  eclipse 15.33%------|\n",
      "|------epoch 1 train error is 0.690590  eclipse 23.01%------|\n",
      "|------epoch 1 train error is 0.709426  eclipse 30.69%------|\n",
      "|------epoch 1 train error is 0.744376  eclipse 38.37%------|\n",
      "|------epoch 1 train error is 0.665399  eclipse 46.05%------|\n",
      "|------epoch 1 train error is 0.714687  eclipse 53.73%------|\n",
      "|------epoch 1 train error is 0.665559  eclipse 61.41%------|\n",
      "|------epoch 1 train error is 0.710789  eclipse 69.09%------|\n",
      "|------epoch 1 train error is 0.665269  eclipse 76.77%------|\n",
      "|------epoch 1 train error is 0.739994  eclipse 84.45%------|\n",
      "|------epoch 1 train error is 0.711733  eclipse 92.13%------|\n",
      "|------epoch 1 train error is 0.685748  eclipse 99.81%------|\n",
      "epcoh 1 dev acc is 68.736667, best dev acc 68.736667\n",
      "|------epoch 2 train error is 0.654842  eclipse 7.65%------|\n",
      "|------epoch 2 train error is 0.569560  eclipse 15.33%------|\n",
      "|------epoch 2 train error is 0.646990  eclipse 23.01%------|\n",
      "|------epoch 2 train error is 0.648183  eclipse 30.69%------|\n",
      "|------epoch 2 train error is 0.592486  eclipse 38.37%------|\n",
      "|------epoch 2 train error is 0.621401  eclipse 46.05%------|\n",
      "|------epoch 2 train error is 0.592247  eclipse 53.73%------|\n",
      "|------epoch 2 train error is 0.611496  eclipse 61.41%------|\n",
      "|------epoch 2 train error is 0.614835  eclipse 69.09%------|\n",
      "|------epoch 2 train error is 0.608561  eclipse 76.77%------|\n",
      "|------epoch 2 train error is 0.648462  eclipse 84.45%------|\n",
      "|------epoch 2 train error is 0.597020  eclipse 92.13%------|\n",
      "|------epoch 2 train error is 0.636317  eclipse 99.81%------|\n",
      "epcoh 2 dev acc is 68.666667, best dev acc 68.736667\n",
      "|------epoch 3 train error is 0.504039  eclipse 7.65%------|\n",
      "|------epoch 3 train error is 0.467373  eclipse 15.33%------|\n",
      "|------epoch 3 train error is 0.503796  eclipse 23.01%------|\n",
      "|------epoch 3 train error is 0.547818  eclipse 30.69%------|\n",
      "|------epoch 3 train error is 0.522135  eclipse 38.37%------|\n",
      "|------epoch 3 train error is 0.535880  eclipse 46.05%------|\n",
      "|------epoch 3 train error is 0.540089  eclipse 53.73%------|\n",
      "|------epoch 3 train error is 0.542061  eclipse 61.41%------|\n",
      "|------epoch 3 train error is 0.531799  eclipse 69.09%------|\n",
      "|------epoch 3 train error is 0.561228  eclipse 76.77%------|\n",
      "|------epoch 3 train error is 0.543282  eclipse 84.45%------|\n",
      "|------epoch 3 train error is 0.565816  eclipse 92.13%------|\n",
      "|------epoch 3 train error is 0.570821  eclipse 99.81%------|\n",
      "epcoh 3 dev acc is 65.423333, best dev acc 68.736667\n",
      "|------epoch 4 train error is 0.403882  eclipse 7.65%------|\n",
      "|------epoch 4 train error is 0.431162  eclipse 15.33%------|\n",
      "|------epoch 4 train error is 0.412466  eclipse 23.01%------|\n",
      "|------epoch 4 train error is 0.452406  eclipse 30.69%------|\n",
      "|------epoch 4 train error is 0.437134  eclipse 38.37%------|\n",
      "|------epoch 4 train error is 0.448544  eclipse 46.05%------|\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "best = 0.0\n",
    "for epoch in range(args.epoch):\n",
    "    train(epoch)\n",
    "    acc = test()\n",
    "    if acc > best:\n",
    "        best = acc\n",
    "        with open(args.save, 'wb') as f:\n",
    "            torch.save(model, f)\n",
    "    print('epcoh {:d} dev acc is {:f}, best dev acc {:f}'.format(epoch, acc, best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
